{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RinadAkel/Convolutions-and-Pooling/blob/main/Convolutions_%26_Pooling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROBW14CBuI8I"
      },
      "source": [
        "Â© 2021 Zaka AI, Inc. All Rights Reserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARofjv2lVTc7"
      },
      "source": [
        "#Convolutional Neural Networks\n",
        "**Objective:** The goal from this exercise is to learn how to build convolutional and pooling neural networks using `Keras`. It includes introducing the `Conv2D` and pooling layers from Keras, building sequential models with stacked layers, padding to fix the border problem and an example on how convolution and padding changes an image matrix.\n",
        "\n",
        "The final part is a hands-on task on building a CNN and training an image classification model on a training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R2pqwPNMnaC"
      },
      "source": [
        "## Stacking Convolutional Layers\n",
        "Keras' `Conv2D` allows you to add convolutional layers to a sequential model similarly to how you would a dense layer. You specify the number of filters, the kernel size and the input shape of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFtydkTSMni7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f284435-8b38-4db6-d10b-bf9859e44fe7"
      },
      "source": [
        "# example of stacked convolutional layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=1, kernel_size=(3,3), input_shape=(8, 8, 1)))\n",
        "model.add(Conv2D(filters=1, kernel_size=(3,3)))\n",
        "# summarize model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 6, 6, 1)           10        \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 4, 4, 1)           10        \n",
            "=================================================================\n",
            "Total params: 20\n",
            "Trainable params: 20\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsNjF_pkP9tI"
      },
      "source": [
        "### Fix border effect problem with padding\n",
        "Padding allows you to keep the information from border pixels and avoid losing them during convolution and decreasing the size of your feature map by adding extra rows and columns of pixels around the borders of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJiSXSKUQFQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb136f8-75cb-4370-cf3f-0ba159ab09ba"
      },
      "source": [
        "# example a convolutional layer with padding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (3,3), padding='same', input_shape=(8, 8, 1)))\n",
        "\n",
        "# summarize model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 8, 8, 1)           10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p4uyWvNRDfc"
      },
      "source": [
        "### Convolutional layer with larger strides\n",
        "Increasing strides makes convolution faster with less operations to fulfill and downsizes the feature map but might cause losing important features from the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MztbZSTiRDn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e771788-06bf-48bb-c684-9880617a5cd2"
      },
      "source": [
        "# example a deep cnn with stride = 2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (3,3), strides=(2, 2), input_shape=(8, 8, 1)))\n",
        "\n",
        "# summarize model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 3, 3, 1)           10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAGQtBOhXW0u"
      },
      "source": [
        "## Pooling Layers\n",
        "Adding pooling layers to the network also downsamples the feature image but without losing as much information as increasing the stride in convolutional layers. The most popular pooling techniques are average pooling and maximum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdsgzdZhXYqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70b2fa0-7edf-432b-e9eb-2bac0fd64e8d"
      },
      "source": [
        "# example a deep cnn with pooling\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (3,3), activation='relu', input_shape=(8, 8, 1)))\n",
        "model.add(AveragePooling2D())\n",
        "\n",
        "# summarize model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 6, 6, 1)           10        \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 3, 3, 1)           0         \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CacGc_bQY9w"
      },
      "source": [
        "### Stacked CNN with padding and pooling\n",
        "You can stack multiple CNN layers with padding to maintain the same image size or shape across the network and after pooling layers equally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYjQPwTaQWle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9d5704-d780-47de-d8d2-4e36db6f4abe"
      },
      "source": [
        "# example a deep cnn with padding and pooling\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (3,3), padding='same', input_shape=(8, 8, 1)))\n",
        "model.add(Conv2D(1, (3,3), padding='same'))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Conv2D(1, (3,3), padding='same'))\n",
        "\n",
        "# summarize model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 1)           10        \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 1)           10        \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 4, 4, 1)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 1)           10        \n",
            "=================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvtsh_zVdIk0"
      },
      "source": [
        "Image output shape is preserved along the convolutions, then halfed in pooling but maintains the shape after final convolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAH7hZPV_ZfF"
      },
      "source": [
        "## Examples of 2D Convolutional layer\n",
        "Let's take an example and see what the output of a single convolution looks like. The data is an image presented in array form and the kernel weights are specified by us to avoid training the network using `set_weights`.\n",
        "\n",
        "To do so, let's create three functions creating three different models as their return:\n",
        "1. `create_oneCNN` which creates a single layer CNN without padding\n",
        "2. `create_model_with_strides` which creates a single layer CNN without padding and a stride set to 2\n",
        "3. `create_model_with_pool` which creates a two-layer CNN with average pooling\n",
        "\n",
        "We can then create a `proNet` function which builds our image array and kernel weights for the model created and shows us the result of passing the image through the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efdbK2igf1eg"
      },
      "source": [
        "First, let's import all the necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpw48y65f4lM"
      },
      "source": [
        "from numpy import asarray\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, AveragePooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHtczPlMeKdI"
      },
      "source": [
        "###1. Single layer CNN without padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPAF1-4mBsjL"
      },
      "source": [
        "def create_oneCNN():\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=1, kernel_size=(3,3), input_shape=(8, 8, 1)))\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQDAfERXehbw"
      },
      "source": [
        "###2. Single layer CNN without padding and stride set to 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrhLnwDzB80A"
      },
      "source": [
        "def create_model_with_strides():\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=1, kernel_size=(3,3), strides=(2,2), input_shape=(8, 8, 1)))\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2fZzBQWfOra"
      },
      "source": [
        "###3. Two-layered network with CNN and pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX8_bLIECGJe"
      },
      "source": [
        "def create_model_with_pool():\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=1, kernel_size=(3,3), input_shape=(8, 8, 1)))\n",
        "  model.add(AveragePooling2D())\n",
        "  print(model.summary())\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lThsD7xxgt8C"
      },
      "source": [
        "###Product Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmIdO5Xhgz10"
      },
      "source": [
        "def proNet(model):\n",
        "  # define input data\n",
        "  data = [[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t\t\t  [0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t\t\t  [0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t\t\t  [0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t\t\t  [0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t\t\t  [0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t\t\t  [0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t\t\t  [0, 0, 0, 1, 1, 0, 0, 0]]\n",
        "  data = asarray(data)\n",
        "  data = data.reshape(1, 8, 8, 1)\n",
        " \n",
        "  # define a vertical line detector\n",
        "  detector = [[[[0]],[[1]],[[0]]],\n",
        "             [[[0]],[[1]],[[0]]],\n",
        "              [[[0]],[[1]],[[0]]]]\n",
        "  weights = [asarray(detector), asarray([0.0])]\n",
        "  print(weights)\n",
        "  # store the weights in the model\n",
        "  model.set_weights(weights)\n",
        "  # apply filter to input data\n",
        "  yhat = model.predict(data)\n",
        "  print(yhat.shape)\n",
        "  for r in range(yhat.shape[1]):\n",
        "\t  # print each column in the row\n",
        "\t  print([yhat[0,r,c,0] for c in range(yhat.shape[2])])\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItsNNaozhhph"
      },
      "source": [
        "###Application\n",
        "Let's apply the convolution and pooling networks architectures and filters we just created on our data using each model we built.\n",
        "\n",
        "Starting with a regular one layer CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LAKEnFih8nF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f46e037-6613-4ab8-c756-44d33e32bd72"
      },
      "source": [
        "model = create_oneCNN()\n",
        "proNet(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 1)           10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "[array([[[[0]],\n",
            "\n",
            "        [[1]],\n",
            "\n",
            "        [[0]]],\n",
            "\n",
            "\n",
            "       [[[0]],\n",
            "\n",
            "        [[1]],\n",
            "\n",
            "        [[0]]],\n",
            "\n",
            "\n",
            "       [[[0]],\n",
            "\n",
            "        [[1]],\n",
            "\n",
            "        [[0]]]]), array([0.])]\n",
            "(1, 6, 6, 1)\n",
            "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pexvl9hCiBqW"
      },
      "source": [
        "CNN with stride"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O_Lrq-siGPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d3e8f99-0a4a-4562-a754-3296ee8d9471"
      },
      "source": [
        "model = create_model_with_strides()\n",
        "proNet(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 1)           10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "[array([[[[0]],\n",
            "\n",
            "        [[1]],\n",
            "\n",
            "        [[0]]],\n",
            "\n",
            "\n",
            "       [[[0]],\n",
            "\n",
            "        [[1]],\n",
            "\n",
            "        [[0]]],\n",
            "\n",
            "\n",
            "       [[[0]],\n",
            "\n",
            "        [[1]],\n",
            "\n",
            "        [[0]]]]), array([0.])]\n",
            "(1, 3, 3, 1)\n",
            "[0.0, 3.0, 0.0]\n",
            "[0.0, 3.0, 0.0]\n",
            "[0.0, 3.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wocJAr3PiLLH"
      },
      "source": [
        "CNN with pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5xBCkYsiNUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe3e0ab9-426a-47fb-d009-35f837a70c04"
      },
      "source": [
        "model = create_model_with_pool()\n",
        "proNet(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 6, 6, 1)           10        \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 3, 3, 1)           0         \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "[array([[[[0]],\n",
            "\n",
            "        [[1]],\n",
            "\n",
            "        [[0]]],\n",
            "\n",
            "\n",
            "       [[[0]],\n",
            "\n",
            "        [[1]],\n",
            "\n",
            "        [[0]]],\n",
            "\n",
            "\n",
            "       [[[0]],\n",
            "\n",
            "        [[1]],\n",
            "\n",
            "        [[0]]]]), array([0.])]\n",
            "(1, 3, 3, 1)\n",
            "[0.0, 3.0, 0.0]\n",
            "[0.0, 3.0, 0.0]\n",
            "[0.0, 3.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIXyZ1NWau34"
      },
      "source": [
        "# Task\n",
        "##CNN and training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhfH0XGxcC1t"
      },
      "source": [
        "Build a CNN and train an image classification model on the data we gathered"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KakpTgUiAxu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cdd8c89-a434-49c0-c2e4-d381289a4ce2"
      },
      "source": [
        "!git clone https://github.com/zaka-ai/computer-vision-course.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'computer-vision-course'...\n",
            "remote: Enumerating objects: 2118, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 2118 (delta 0), reused 0 (delta 0), pack-reused 2115\u001b[K\n",
            "Receiving objects: 100% (2118/2118), 51.06 MiB | 31.79 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yry18yBayWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14181dbb-40ef-448d-c591-cc4820d284fb"
      },
      "source": [
        "# FILL BLANKS\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, AveragePooling2D, Flatten\n",
        "\n",
        "datagen = ImageDataGenerator(vertical_flip=True, rotation_range=90, brightness_range=[0.3, 1.5])\n",
        "\n",
        "train_it = datagen.flow_from_directory(\"computer-vision-course/deep_learning/dataset/train\", batch_size=2, target_size=(256, 256), class_mode=\"binary\")\n",
        "eval_it = datagen.flow_from_directory(\"computer-vision-course/deep_learning/dataset/validation\", batch_size=1, target_size=(256, 256), class_mode=\"binary\")\n",
        "\n",
        "# define CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(4, (3,3), input_shape=(256, 256, 3), activation=\"relu\"))\n",
        "model.add(AveragePooling2D())\n",
        "model.add(Conv2D(8, (3,3), activation=\"relu\"))\n",
        "model.add(AveragePooling2D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_it, epochs=50, steps_per_epoch=10, validation_data=eval_it)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 254, 254, 4)       112       \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 127, 127, 4)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 125, 125, 8)       296       \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 62, 62, 8)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 30752)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 50)                1537650   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,538,109\n",
            "Trainable params: 1,538,109\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "10/10 [==============================] - 29s 134ms/step - loss: 1309.5339 - accuracy: 0.4000 - val_loss: 443.2159 - val_accuracy: 0.5000\n",
            "Epoch 2/50\n",
            "10/10 [==============================] - 1s 114ms/step - loss: 931.5126 - accuracy: 0.3500 - val_loss: 185.6507 - val_accuracy: 0.6000\n",
            "Epoch 3/50\n",
            "10/10 [==============================] - 1s 115ms/step - loss: 170.0469 - accuracy: 0.7000 - val_loss: 247.9707 - val_accuracy: 0.6000\n",
            "Epoch 4/50\n",
            "10/10 [==============================] - 1s 112ms/step - loss: 51.0771 - accuracy: 0.7000 - val_loss: 283.5395 - val_accuracy: 0.5000\n",
            "Epoch 5/50\n",
            "10/10 [==============================] - 1s 114ms/step - loss: 110.2960 - accuracy: 0.7000 - val_loss: 25.5491 - val_accuracy: 0.9000\n",
            "Epoch 6/50\n",
            "10/10 [==============================] - 1s 116ms/step - loss: 53.5927 - accuracy: 0.8000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 7/50\n",
            "10/10 [==============================] - 1s 115ms/step - loss: 3.7639 - accuracy: 0.9500 - val_loss: 95.2844 - val_accuracy: 0.6000\n",
            "Epoch 8/50\n",
            "10/10 [==============================] - 1s 127ms/step - loss: 10.8082 - accuracy: 0.8000 - val_loss: 23.9744 - val_accuracy: 0.8000\n",
            "Epoch 9/50\n",
            "10/10 [==============================] - 1s 113ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 14.4703 - val_accuracy: 0.9000\n",
            "Epoch 10/50\n",
            "10/10 [==============================] - 1s 113ms/step - loss: 1.7139e-21 - accuracy: 1.0000 - val_loss: 13.6190 - val_accuracy: 0.9000\n",
            "Epoch 11/50\n",
            "10/10 [==============================] - 1s 111ms/step - loss: 4.7513e-28 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "10/10 [==============================] - 1s 107ms/step - loss: 1.8356e-37 - accuracy: 1.0000 - val_loss: 1.2398e-32 - val_accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "10/10 [==============================] - 1s 113ms/step - loss: 2.3044e-04 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "10/10 [==============================] - 1s 112ms/step - loss: 2.6713e-33 - accuracy: 1.0000 - val_loss: 1.2015e-26 - val_accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "10/10 [==============================] - 1s 115ms/step - loss: 2.1548e-12 - accuracy: 1.0000 - val_loss: 4.1071 - val_accuracy: 0.9000\n",
            "Epoch 16/50\n",
            "10/10 [==============================] - 1s 112ms/step - loss: 8.6459e-31 - accuracy: 1.0000 - val_loss: 8.7193e-36 - val_accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "10/10 [==============================] - 1s 113ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 2.8301e-29 - val_accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "10/10 [==============================] - 1s 115ms/step - loss: 2.8131e-27 - accuracy: 1.0000 - val_loss: 2.3687 - val_accuracy: 0.9000\n",
            "Epoch 19/50\n",
            "10/10 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 3.8964e-25 - val_accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "10/10 [==============================] - 1s 111ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "10/10 [==============================] - 1s 113ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "10/10 [==============================] - 1s 110ms/step - loss: 0.0697 - accuracy: 0.9500 - val_loss: 26.9253 - val_accuracy: 0.8000\n",
            "Epoch 23/50\n",
            "10/10 [==============================] - 1s 111ms/step - loss: 6.7673 - accuracy: 0.9500 - val_loss: 15.0087 - val_accuracy: 0.9000\n",
            "Epoch 24/50\n",
            "10/10 [==============================] - 1s 115ms/step - loss: 2.0350e-33 - accuracy: 1.0000 - val_loss: 7.0737e-37 - val_accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "10/10 [==============================] - 1s 111ms/step - loss: 3.8348e-27 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "10/10 [==============================] - 1s 115ms/step - loss: 9.9296e-29 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "10/10 [==============================] - 1s 112ms/step - loss: 1.4071e-35 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "10/10 [==============================] - 1s 110ms/step - loss: 1.7621e-28 - accuracy: 1.0000 - val_loss: 2.0200e-32 - val_accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "10/10 [==============================] - 1s 115ms/step - loss: 8.8377e-18 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "10/10 [==============================] - 1s 110ms/step - loss: 4.5876e-22 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "10/10 [==============================] - 1s 110ms/step - loss: 7.4611e-25 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "10/10 [==============================] - 1s 114ms/step - loss: 1.0706e-21 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "10/10 [==============================] - 1s 117ms/step - loss: 2.4199e-16 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "10/10 [==============================] - 1s 119ms/step - loss: 7.7421e-29 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "10/10 [==============================] - 1s 119ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "10/10 [==============================] - 1s 120ms/step - loss: 4.3810e-21 - accuracy: 1.0000 - val_loss: 2.6586e-31 - val_accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "10/10 [==============================] - 1s 118ms/step - loss: 1.3034e-28 - accuracy: 1.0000 - val_loss: 4.0996e-33 - val_accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "10/10 [==============================] - 1s 120ms/step - loss: 6.4651e-18 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "10/10 [==============================] - 1s 119ms/step - loss: 1.4272e-27 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "10/10 [==============================] - 1s 117ms/step - loss: 1.6406e-27 - accuracy: 1.0000 - val_loss: 2.8310e-25 - val_accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "10/10 [==============================] - 1s 118ms/step - loss: 1.0210e-26 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "10/10 [==============================] - 1s 123ms/step - loss: 9.3042e-28 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "10/10 [==============================] - 1s 123ms/step - loss: 4.8242e-17 - accuracy: 1.0000 - val_loss: 1.0463e-37 - val_accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "10/10 [==============================] - 1s 121ms/step - loss: 6.3848e-23 - accuracy: 1.0000 - val_loss: 1.1658e-34 - val_accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "10/10 [==============================] - 1s 118ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "10/10 [==============================] - 1s 119ms/step - loss: 5.9845e-16 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "10/10 [==============================] - 1s 116ms/step - loss: 1.2047e-10 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "10/10 [==============================] - 1s 120ms/step - loss: 4.7795e-15 - accuracy: 1.0000 - val_loss: 1.5270e-25 - val_accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "10/10 [==============================] - 1s 119ms/step - loss: 2.3475e-36 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6dd264d6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}